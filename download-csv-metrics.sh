#!/bin/bash

# download-csv-metrics.sh
# Download performance metrics CSV files from S3 or extract from CloudWatch Logs

set -e

# Configuration
BUCKET_NAME="home-sunhao"
REGION="us-east-2"
S3_PREFIX="lambda-metrics"
OUTPUT_DIR="./performance-metrics"

# Worker definitions: WorkerType:CSVName:LambdaFunctionName
WORKERS=(
    "ScanWorker:scan:pixels-scan-worker"
    "AggregationWorker:aggregation:pixels-aggregationworker"
    "PartitionWorker:partition:pixels-partitionworker"
    "BroadcastJoinWorker:broadcast_join:pixels-broadcastjoinworker"
    "PartitionedJoinWorker:partitioned_join:pixels-partitionedjoinworker"
    "BroadcastChainJoinWorker:broadcast_chain_join:pixels-broadcastchainjoinworker"
    "PartitionedChainJoinWorker:partitioned_chain_join:pixels-partitionedchainjoinworker"
)

# Colors
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m'

echo ""
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘        ä¸‹è½½æ€§èƒ½æŒ‡æ ‡ CSV æ–‡ä»¶                               â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# Create output directory
mkdir -p "$OUTPUT_DIR"

# Function to download from S3
download_from_s3() {
    local WORKER_TYPE=$1
    local CSV_NAME=$2
    local S3_KEY="${S3_PREFIX}/${CSV_NAME}_performance_metrics.csv"
    local OUTPUT_FILE="${OUTPUT_DIR}/${CSV_NAME}_performance_metrics.csv"
    
    echo -e "${BLUE}[ä¸‹è½½ ${WORKER_TYPE}]${NC}"
    echo "  S3 è·¯å¾„: s3://${BUCKET_NAME}/${S3_KEY}"
    echo "  æœ¬åœ°è·¯å¾„: ${OUTPUT_FILE}"
    
    if aws s3 ls "s3://${BUCKET_NAME}/${S3_KEY}" --region $REGION > /dev/null 2>&1; then
        aws s3 cp "s3://${BUCKET_NAME}/${S3_KEY}" "$OUTPUT_FILE" --region $REGION
        echo -e "${GREEN}âœ… ä¸‹è½½æˆåŠŸ${NC}"
        return 0
    else
        echo -e "${YELLOW}âš ï¸  S3 æ–‡ä»¶ä¸å­˜åœ¨${NC}"
        return 1
    fi
}

# Function to extract from CloudWatch Logs and save as CSV
extract_from_logs() {
    local WORKER_TYPE=$1
    local CSV_NAME=$2
    local LAMBDA_NAME=$3
    local LOG_GROUP="/aws/lambda/${LAMBDA_NAME}"
    local OUTPUT_FILE="${OUTPUT_DIR}/${CSV_NAME}_performance_metrics_from_logs.csv"
    
    echo -e "${BLUE}[ä» CloudWatch Logs æå– ${WORKER_TYPE}]${NC}"
    echo "  Log Group: ${LOG_GROUP}"
    echo "  è¾“å‡ºæ–‡ä»¶: ${OUTPUT_FILE}"
    
    # Check if log group exists
    LOG_GROUP_EXISTS=$(aws logs describe-log-groups --log-group-name-prefix "/aws/lambda/${LAMBDA_NAME}" --region $REGION --query 'logGroups[?logGroupName==`'"$LOG_GROUP"'`].logGroupName' --output text 2>/dev/null || echo "")
    if [ -z "$LOG_GROUP_EXISTS" ] || [ "$LOG_GROUP_EXISTS" = "None" ]; then
        echo -e "${YELLOW}âš ï¸  Log Group ä¸å­˜åœ¨: ${LOG_GROUP}${NC}"
        echo "  æç¤º: Lambda å‡½æ•°å¯èƒ½å°šæœªè¢«è°ƒç”¨"
        return 1
    fi
    
    # Get recent log events (last 24 hours)
    START_TIME=$(($(date +%s) - 86400))000
    
    # Create CSV file with header
    echo "Timestamp,WorkerType,ReadTimeMs,ComputeTimeMs,WriteCacheTimeMs,WriteFileTimeMs,ComputePct,WriteCachePct,WriteFilePct,S3StoragePct" > "$OUTPUT_FILE"
    
    # Extract metrics from logs
    LOG_EVENTS=$(aws logs filter-log-events \
        --log-group-name "$LOG_GROUP" \
        --start-time $START_TIME \
        --region $REGION \
        --filter-pattern "Four-Stage Performance Metrics" \
        --query 'events[*].[timestamp,message]' \
        --output text 2>/dev/null || echo "")
    
    if [ -z "$LOG_EVENTS" ] || [ "$LOG_EVENTS" = "None" ]; then
        echo -e "${YELLOW}âš ï¸  æœªæ‰¾åˆ°æ€§èƒ½æŒ‡æ ‡æ—¥å¿—${NC}"
        return 1
    fi
    
    COUNT=0
    echo "$LOG_EVENTS" | while IFS=$'\t' read -r timestamp message; do
        if [ -n "$timestamp" ] && [ -n "$message" ]; then
            # Parse metrics from log message
            READ_MS=$(echo "$message" | grep -oE 'READ=[0-9]+' | grep -oE '[0-9]+' || echo "0")
            COMPUTE_MS=$(echo "$message" | grep -oE 'COMPUTE=[0-9]+' | grep -oE '[0-9]+' || echo "0")
            WRITE_CACHE_MS=$(echo "$message" | grep -oE 'WRITE_CACHE=[0-9]+' | grep -oE '[0-9]+' || echo "0")
            WRITE_FILE_MS=$(echo "$message" | grep -oE 'WRITE_FILE=[0-9]+' | grep -oE '[0-9]+' || echo "0")
            
            # Get corresponding percentages
            REQUEST_ID=$(echo "$message" | grep -oE '\[[0-9a-f-]+\]' | head -1 | tr -d '[]')
            PERCENTAGES=$(aws logs filter-log-events \
                --log-group-name "$LOG_GROUP" \
                --start-time $START_TIME \
                --region $REGION \
                --filter-pattern "\"$REQUEST_ID\" Percentages" \
                --query 'events[0].message' \
                --output text 2>/dev/null || echo "")
            
            if [ -n "$PERCENTAGES" ] && [ "$PERCENTAGES" != "None" ]; then
                COMPUTE_PCT=$(echo "$PERCENTAGES" | grep -oE 'COMPUTE=[0-9.]+' | grep -oE '[0-9.]+' || echo "0.00")
                WRITE_CACHE_PCT=$(echo "$PERCENTAGES" | grep -oE 'WRITE_CACHE=[0-9.]+' | grep -oE '[0-9.]+' || echo "0.00")
                WRITE_FILE_PCT=$(echo "$PERCENTAGES" | grep -oE 'WRITE_FILE=[0-9.]+' | grep -oE '[0-9.]+' || echo "0.00")
                S3_STORAGE_PCT=$(echo "$PERCENTAGES" | grep -oE 'S3 Storage=[0-9.]+' | grep -oE '[0-9.]+' || echo "0.00")
            else
                # Calculate from raw values
                TOTAL_MS=$((READ_MS + COMPUTE_MS + WRITE_CACHE_MS + WRITE_FILE_MS))
                if [ $TOTAL_MS -gt 0 ]; then
                    COMPUTE_PCT=$(awk "BEGIN {printf \"%.2f\", ($COMPUTE_MS / $TOTAL_MS) * 100}")
                    WRITE_CACHE_PCT=$(awk "BEGIN {printf \"%.2f\", ($WRITE_CACHE_MS / $TOTAL_MS) * 100}")
                    WRITE_FILE_PCT=$(awk "BEGIN {printf \"%.2f\", ($WRITE_FILE_MS / $TOTAL_MS) * 100}")
                    S3_STORAGE_PCT=$(awk "BEGIN {printf \"%.2f\", (($READ_MS + $WRITE_FILE_MS) / $TOTAL_MS) * 100}")
                else
                    COMPUTE_PCT="0.00"
                    WRITE_CACHE_PCT="0.00"
                    WRITE_FILE_PCT="0.00"
                    S3_STORAGE_PCT="0.00"
                fi
            fi
            
            # Convert timestamp to milliseconds (if needed)
            TIMESTAMP_MS=$timestamp
            if [ ${#TIMESTAMP_MS} -eq 10 ]; then
                TIMESTAMP_MS=$((TIMESTAMP_MS * 1000))
            fi
            
            # Write CSV row
            echo "${TIMESTAMP_MS},${WORKER_TYPE},${READ_MS},${COMPUTE_MS},${WRITE_CACHE_MS},${WRITE_FILE_MS},${COMPUTE_PCT},${WRITE_CACHE_PCT},${WRITE_FILE_PCT},${S3_STORAGE_PCT}" >> "$OUTPUT_FILE"
            COUNT=$((COUNT + 1))
        fi
    done
    
    # Count actual lines (subtract header)
    ACTUAL_COUNT=$(wc -l < "$OUTPUT_FILE" | tr -d ' ')
    ACTUAL_COUNT=$((ACTUAL_COUNT - 1))
    
    if [ $ACTUAL_COUNT -gt 0 ]; then
        echo -e "${GREEN}âœ… æå–æˆåŠŸ: ${ACTUAL_COUNT} æ¡è®°å½•${NC}"
        return 0
    else
        echo -e "${YELLOW}âš ï¸  æœªæå–åˆ°æ•°æ®${NC}"
        rm -f "$OUTPUT_FILE"
        return 1
    fi
}

# Try to download from S3 first, then extract from logs if not found
SUCCESS_COUNT=0
FAIL_COUNT=0

for WORKER_DEF in "${WORKERS[@]}"; do
    WORKER_TYPE="${WORKER_DEF%%:*}"
    REMAINING="${WORKER_DEF#*:}"
    CSV_NAME="${REMAINING%%:*}"
    LAMBDA_NAME="${REMAINING##*:}"
    
    echo ""
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    
    if download_from_s3 "$WORKER_TYPE" "$CSV_NAME"; then
        SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
    else
        echo "  å°è¯•ä» CloudWatch Logs æå–..."
        if extract_from_logs "$WORKER_TYPE" "$CSV_NAME" "$LAMBDA_NAME"; then
            SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
        else
            FAIL_COUNT=$((FAIL_COUNT + 1))
        fi
    fi
done

echo ""
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘                     ä¸‹è½½å®Œæˆ                               â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "ç»Ÿè®¡:"
echo -e "  ${GREEN}âœ… æˆåŠŸ: $SUCCESS_COUNT${NC}"
if [ $FAIL_COUNT -gt 0 ]; then
    echo -e "  ${RED}âŒ å¤±è´¥: $FAIL_COUNT${NC}"
fi
echo ""
echo "æ–‡ä»¶ä½ç½®:"
echo "  ${OUTPUT_DIR}/"
echo ""
ls -lh "$OUTPUT_DIR"/*.csv 2>/dev/null | awk '{print "  " $9 " (" $5 ")"}'
echo ""
echo "ğŸ’¡ æç¤º:"
echo "  å¦‚æœ CSV æ–‡ä»¶åœ¨ S3 ä¸­ä¸å­˜åœ¨ï¼Œè„šæœ¬ä¼šä» CloudWatch Logs æå–æ•°æ®"
echo "  è¦å¯ç”¨è‡ªåŠ¨ä¸Šä¼ åˆ° S3ï¼Œè¯·ä¿®æ”¹ WorkerMetrics.java å¹¶é‡æ–°éƒ¨ç½²"
echo ""

