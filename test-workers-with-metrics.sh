#!/bin/bash

# test-workers-with-metrics.sh
# Test all Pixels Lambda Workers and extract performance metrics in the same format

set -e

# Configuration
BUCKET_NAME="home-sunhao"
REGION="us-east-2"

# Colors
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m'

echo ""
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘    æµ‹è¯•æ‰€æœ‰ Pixels Lambda Workers å¹¶æå–æ€§èƒ½æŒ‡æ ‡          â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# Check which Lambda functions exist
echo "ğŸ“‹ æ£€æŸ¥å·²éƒ¨ç½²çš„ Lambda å‡½æ•°..."
FUNCTIONS=$(aws lambda list-functions --region $REGION \
    --query 'Functions[?contains(FunctionName, `pixels`) || contains(FunctionName, `worker`)].FunctionName' \
    --output text 2>/dev/null || echo "")

if [ -z "$FUNCTIONS" ]; then
    echo -e "${RED}âŒ æœªæ‰¾åˆ°ä»»ä½• Pixels Worker Lambda å‡½æ•°${NC}"
    exit 1
fi

echo -e "${GREEN}âœ… æ‰¾åˆ°ä»¥ä¸‹ Lambda å‡½æ•°:${NC}"
echo "$FUNCTIONS" | tr '\t' '\n' | while read func; do
    echo "   - $func"
done
echo ""

# Extract performance metrics from CloudWatch Logs
extract_performance_metrics() {
    local FUNCTION_NAME=$1
    local WORKER_TYPE=$2
    local LOG_GROUP="/aws/lambda/${FUNCTION_NAME}"
    
    echo ""
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo -e "${GREEN}=== ${WORKER_TYPE} æ€§èƒ½æŒ‡æ ‡æ‘˜è¦ ===${NC}"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    
    # Get recent log events with performance metrics (last 10 minutes)
    START_TIME=$(($(date +%s) - 600))000
    
    # Get the most recent performance metrics log entry
    METRICS_LINE=$(aws logs tail "$LOG_GROUP" --since 10m --region $REGION --format short 2>/dev/null \
        | grep "Four-Stage Performance Metrics" | tail -1)
    
    # Get the most recent percentages log entry
    PERCENTAGES_LINE=$(aws logs tail "$LOG_GROUP" --since 10m --region $REGION --format short 2>/dev/null \
        | grep "Percentages:" | tail -1)
    
    # Extract the message part (after timestamp and request ID)
    METRICS=$(echo "$METRICS_LINE" | sed -n 's/.*Four-Stage Performance Metrics (ms): //p' || echo "")
    PERCENTAGES=$(echo "$PERCENTAGES_LINE" | sed -n 's/.*Percentages: //p' || echo "")
    
    if [ -n "$METRICS" ] && [ -n "$METRICS_LINE" ]; then
        # Parse metrics
        READ_MS=$(echo "$METRICS" | grep -oE 'READ=[0-9]+' | grep -oE '[0-9]+' || echo "0")
        COMPUTE_MS=$(echo "$METRICS" | grep -oE 'COMPUTE=[0-9]+' | grep -oE '[0-9]+' || echo "0")
        WRITE_CACHE_MS=$(echo "$METRICS" | grep -oE 'WRITE_CACHE=[0-9]+' | grep -oE '[0-9]+' || echo "0")
        WRITE_FILE_MS=$(echo "$METRICS" | grep -oE 'WRITE_FILE=[0-9]+' | grep -oE '[0-9]+' || echo "0")
        
        # Parse percentages
        if [ -n "$PERCENTAGES" ] && [ "$PERCENTAGES" != "None" ] && [ "$PERCENTAGES" != "null" ]; then
            COMPUTE_PCT=$(echo "$PERCENTAGES" | grep -oE 'COMPUTE=[0-9.]+' | grep -oE '[0-9.]+' || echo "0.00")
            WRITE_CACHE_PCT=$(echo "$PERCENTAGES" | grep -oE 'WRITE_CACHE=[0-9.]+' | grep -oE '[0-9.]+' || echo "0.00")
            WRITE_FILE_PCT=$(echo "$PERCENTAGES" | grep -oE 'WRITE_FILE=[0-9.]+' | grep -oE '[0-9.]+' || echo "0.00")
            S3_STORAGE_PCT=$(echo "$PERCENTAGES" | grep -oE 'S3 Storage=[0-9.]+' | grep -oE '[0-9.]+' || echo "0.00")
        else
            # Calculate percentages if not found in log
            TOTAL_MS=$((READ_MS + COMPUTE_MS + WRITE_CACHE_MS + WRITE_FILE_MS))
            if [ $TOTAL_MS -gt 0 ]; then
                COMPUTE_PCT=$(awk "BEGIN {printf \"%.2f\", ($COMPUTE_MS / $TOTAL_MS) * 100}")
                WRITE_CACHE_PCT=$(awk "BEGIN {printf \"%.2f\", ($WRITE_CACHE_MS / $TOTAL_MS) * 100}")
                WRITE_FILE_PCT=$(awk "BEGIN {printf \"%.2f\", ($WRITE_FILE_MS / $TOTAL_MS) * 100}")
                S3_STORAGE_PCT=$(awk "BEGIN {printf \"%.2f\", (($READ_MS + $WRITE_FILE_MS) / $TOTAL_MS) * 100}")
            else
                COMPUTE_PCT="0.00"
                WRITE_CACHE_PCT="0.00"
                WRITE_FILE_PCT="0.00"
                S3_STORAGE_PCT="0.00"
            fi
        fi
        
        TOTAL_MS=$((READ_MS + COMPUTE_MS + WRITE_CACHE_MS + WRITE_FILE_MS))
        S3_STORAGE_MS=$((READ_MS + WRITE_FILE_MS))
        
        # Display in the same format as lambda-full-execution-log.txt
        echo "Worker: ${WORKER_TYPE}"
        echo "READ: ${READ_MS} ms"
        echo "COMPUTE: ${COMPUTE_MS} ms (${COMPUTE_PCT}%)"
        echo "WRITE_CACHE: ${WRITE_CACHE_MS} ms (${WRITE_CACHE_PCT}%)"
        echo "WRITE_FILE: ${WRITE_FILE_MS} ms (${WRITE_FILE_PCT}%)"
        echo "S3 Storage (READ + WRITE_FILE): ${S3_STORAGE_PCT}% (${S3_STORAGE_MS} ms = ${READ_MS} ms + ${WRITE_FILE_MS} ms)"
        TOTAL_SEC=$(echo "scale=1; $TOTAL_MS / 1000" | bc 2>/dev/null || echo "$(($TOTAL_MS / 1000)).0")
        echo "æ€»è€—æ—¶: ${TOTAL_MS} ms (çº¦ ${TOTAL_SEC} ç§’)"
        
        # Get memory usage from REPORT log
        MEMORY_INFO=$(aws logs filter-log-events \
            --log-group-name "$LOG_GROUP" \
            --start-time $START_TIME \
            --region $REGION \
            --filter-pattern "REPORT" \
            --query 'events[-1].message' \
            --output text 2>/dev/null || echo "")
        
        if [ -n "$MEMORY_INFO" ] && [ "$MEMORY_INFO" != "None" ]; then
            MEMORY_USED=$(echo "$MEMORY_INFO" | grep -oE 'Max Memory Used: [0-9]+' | grep -oE '[0-9]+' || echo "")
            MEMORY_SIZE=$(echo "$MEMORY_INFO" | grep -oE 'Memory Size: [0-9]+' | grep -oE '[0-9]+' || echo "")
            if [ -n "$MEMORY_USED" ] && [ -n "$MEMORY_SIZE" ]; then
                echo "å†…å­˜ä½¿ç”¨: ${MEMORY_USED} MB / ${MEMORY_SIZE} MB"
            fi
        fi
        
    else
        echo -e "${YELLOW}âš ï¸  æœªæ‰¾åˆ°æœ€è¿‘çš„æ€§èƒ½æŒ‡æ ‡${NC}"
        echo "   å¯èƒ½çš„åŸå› :"
        echo "   1. å‡½æ•°å°šæœªæ‰§è¡Œ"
        echo "   2. æ‰§è¡Œæ—¶é—´è¶…è¿‡ 5 åˆ†é’Ÿ"
        echo "   3. æ—¥å¿—å°šæœªç”Ÿæˆ"
        echo ""
        echo "   æç¤º: å…ˆè°ƒç”¨å‡½æ•°ï¼Œç„¶åç«‹å³è¿è¡Œæ­¤è„šæœ¬æå–æŒ‡æ ‡"
    fi
    
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo ""
}

# Test ScanWorker
test_scan_worker() {
    echo -e "${BLUE}[æµ‹è¯• ScanWorker]${NC}"
    
    if echo "$FUNCTIONS" | grep -q "pixels-scan-worker"; then
        echo "è°ƒç”¨ ScanWorker..."
        
        cat > /tmp/test-scan-input.json << JSON
{
  "transId": 12345,
  "timestamp": -1,
  "requestId": "test-scan-$(date +%s)",
  "tableInfo": {
    "tableName": "test_table",
    "base": true,
    "columnsToRead": ["col1", "col2", "col3"],
    "storageInfo": {
      "scheme": "s3",
      "endpoint": "https://s3.${REGION}.amazonaws.com"
    },
    "inputSplits": [
      {
        "inputInfos": [
          {
            "inputId": 1,
            "path": "s3://${BUCKET_NAME}/test-data/large_test_data.pxl",
            "rgStart": 0,
            "rgLength": -1,
            "storageInfo": {
              "scheme": "s3",
              "endpoint": "https://s3.${REGION}.amazonaws.com"
            }
          }
        ]
      }
    ],
    "filter": "{\"schemaName\":\"test\",\"tableName\":\"test_table\",\"columnFilters\":{}}"
  },
  "scanProjection": [true, true, true],
  "partialAggregationPresent": false,
  "partialAggregationInfo": null,
  "output": {
    "path": "s3://${BUCKET_NAME}/output/",
    "fileNames": ["scan_result.pxl"],
    "storageInfo": {
      "scheme": "s3",
      "endpoint": "https://s3.${REGION}.amazonaws.com"
    },
    "encoding": true
  },
  "inputStorageInfo": {
    "scheme": "s3",
    "endpoint": "https://s3.${REGION}.amazonaws.com"
  }
}
JSON

        aws lambda invoke \
            --function-name pixels-scan-worker \
            --payload file:///tmp/test-scan-input.json \
            --cli-binary-format raw-in-base64-out \
            --region $REGION \
            /tmp/scan-response.json > /dev/null 2>&1
        
        echo "ç­‰å¾…æ—¥å¿—ç”Ÿæˆ..."
        sleep 5
        
        # Extract performance metrics
        extract_performance_metrics "pixels-scan-worker" "ScanWorker"
        
        # Save to file
        OUTPUT_FILE="lambda-worker-metrics-summary.txt"
        {
            echo "=== ScanWorker æ€§èƒ½æŒ‡æ ‡æ‘˜è¦ ==="
            echo "Worker: ScanWorker"
            echo "æå–æ—¶é—´: $(date)"
            echo ""
        } > "$OUTPUT_FILE"
        extract_performance_metrics "pixels-scan-worker" "ScanWorker" >> "$OUTPUT_FILE" 2>&1 || true
        
    else
        echo -e "${YELLOW}âš ï¸  pixels-scan-worker å‡½æ•°ä¸å­˜åœ¨ï¼Œè·³è¿‡${NC}"
    fi
}

# Main execution
test_scan_worker

echo ""
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo -e "${GREEN}âœ… æµ‹è¯•å®Œæˆ${NC}"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "ğŸ“ å…³äºæ€§èƒ½æŒ‡æ ‡æ–‡ä»¶ä½ç½®:"
echo ""
echo "   âš ï¸  æ€§èƒ½æŒ‡æ ‡ CSV æ–‡ä»¶ä¿å­˜åœ¨ Lambda å‡½æ•°çš„è¿è¡Œæ—¶ç¯å¢ƒä¸­ï¼Œä¸æ˜¯æ‚¨çš„ Macï¼"
echo ""
echo "   æ–‡ä»¶ä½ç½®ï¼ˆåœ¨ Lambda è¿è¡Œç¯å¢ƒå†…ï¼‰:"
echo "   - ScanWorker: /tmp/scan_performance_metrics.csv"
echo "   - AggregationWorker: /tmp/aggregation_performance_metrics.csv"
echo "   - PartitionWorker: /tmp/partition_performance_metrics.csv"
echo "   - BroadcastJoinWorker: /tmp/broadcast_join_performance_metrics.csv"
echo "   - PartitionedJoinWorker: /tmp/partitioned_join_performance_metrics.csv"
echo ""
echo "   ğŸ’¡ è¿™äº›æ–‡ä»¶æ— æ³•ç›´æ¥ä» Mac è®¿é—®ï¼Œä½†æ€§èƒ½æŒ‡æ ‡å·²è¾“å‡ºåˆ° CloudWatch Logs"
echo "   æ‚¨å¯ä»¥:"
echo "   1. ä» CloudWatch Logs æå–ï¼ˆå·²åœ¨æ­¤è„šæœ¬ä¸­å®ç°ï¼‰"
echo "   2. ä¿®æ”¹ä»£ç å°† CSV æ–‡ä»¶ä¸Šä¼ åˆ° S3"
echo "   3. åœ¨ AWS Console ä¸­æŸ¥çœ‹ CloudWatch Logs"
echo ""

