#!/bin/bash
# ä¸Šä¼ æµ‹è¯•æ•°æ®æ–‡ä»¶åˆ° S3

set -e
set -o pipefail

# é…ç½®
BUCKET_NAME="home-sunhao"
LAMBDA_REGION="us-east-2"
REGION="$LAMBDA_REGION"
LOCAL_DATA_DIR="/Users/sunhao/Documents/pixels/test/test_datasource"
S3_TEST_DATA_PREFIX="test-data/workers-performance"

# é¢œè‰²è¾“å‡º
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log_info() {
    echo -e "${BLUE}â„¹ï¸  [INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}âœ… [SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}âš ï¸  [WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}âŒ [ERROR]${NC} $1"
}

# æ£€æŸ¥æœ¬åœ°æ–‡ä»¶ç›®å½•
if [ ! -d "$LOCAL_DATA_DIR" ]; then
    log_error "æœ¬åœ°æ•°æ®ç›®å½•ä¸å­˜åœ¨: $LOCAL_DATA_DIR"
    exit 1
fi

echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
log_info "ä¸Šä¼ æµ‹è¯•æ•°æ®åˆ° S3"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "ğŸ“ æœ¬åœ°ç›®å½•: $LOCAL_DATA_DIR"
echo "â˜ï¸  S3 è·¯å¾„: s3://$BUCKET_NAME/$S3_TEST_DATA_PREFIX/"
echo "ğŸŒ åŒºåŸŸ: $REGION"
echo ""

# å®šä¹‰æ–‡ä»¶åˆ—è¡¨åŠè¯´æ˜ï¼ˆä½¿ç”¨å‡½æ•°ä»£æ›¿å…³è”æ•°ç»„ï¼‰
get_file_desc() {
    case "$1" in
        "AggregationWorker_data.pxl")
            echo "Aggregation Worker (56M)"
            ;;
        "BroadcastJoinWorker_data1.pxl")
            echo "Broadcast Join Worker - å¤§è¡¨ (5.1M)"
            ;;
        "BroadcastJoinWorker_data2.pxl")
            echo "Broadcast Join Worker - å°è¡¨ (1.4M)"
            ;;
        "PartitionWorker_data.pxl")
            echo "Partition Worker (54M)"
            ;;
        "PartitionedJoinWorker_data1.pxl")
            echo "Partitioned Join Worker - å¤§è¡¨ (4.8M)"
            ;;
        "PartitionedJoinWorker_data2.pxl")
            echo "Partitioned Join Worker - å°è¡¨ (1.2M)"
            ;;
        "ScanWorker_data.pxl")
            echo "Scan Worker (49M)"
            ;;
        *)
            echo "$1"
            ;;
    esac
}

FILES=(
    "AggregationWorker_data.pxl"
    "BroadcastJoinWorker_data1.pxl"
    "BroadcastJoinWorker_data2.pxl"
    "PartitionWorker_data.pxl"
    "PartitionedJoinWorker_data1.pxl"
    "PartitionedJoinWorker_data2.pxl"
    "ScanWorker_data.pxl"
)

# ç»Ÿè®¡å˜é‡
TOTAL_FILES=${#FILES[@]}
SUCCESS_COUNT=0
FAIL_COUNT=0
SKIP_COUNT=0
TOTAL_SIZE=0

# ä¸Šä¼ æ–‡ä»¶
for i in "${!FILES[@]}"; do
    FILE="${FILES[$i]}"
    LOCAL_PATH="$LOCAL_DATA_DIR/$FILE"
    S3_PATH="s3://$BUCKET_NAME/$S3_TEST_DATA_PREFIX/$FILE"
    DESC=$(get_file_desc "$FILE")
    
    echo "[$((i+1))/$TOTAL_FILES] $DESC"
    
    # æ£€æŸ¥æœ¬åœ°æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if [ ! -f "$LOCAL_PATH" ]; then
        log_warning "  æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: $FILE"
        SKIP_COUNT=$((SKIP_COUNT + 1))
        echo ""
        continue
    fi
    
    # è·å–æ–‡ä»¶å¤§å°
    if command -v stat > /dev/null 2>&1; then
        if [[ "$OSTYPE" == "darwin"* ]]; then
            FILE_SIZE=$(stat -f%z "$LOCAL_PATH" 2>/dev/null || echo "0")
        else
            FILE_SIZE=$(stat -c%s "$LOCAL_PATH" 2>/dev/null || echo "0")
        fi
    else
        FILE_SIZE=0
    fi
    
    if [ "$FILE_SIZE" -gt 0 ]; then
        FILE_SIZE_MB=$(echo "scale=2; $FILE_SIZE / 1024 / 1024" | bc 2>/dev/null || echo "0")
        log_info "  æ–‡ä»¶å¤§å°: ${FILE_SIZE_MB} MB"
        TOTAL_SIZE=$((TOTAL_SIZE + FILE_SIZE))
    fi
    
    # æ£€æŸ¥ S3 æ˜¯å¦å·²å­˜åœ¨
    if aws s3 ls "$S3_PATH" --region "$REGION" > /dev/null 2>&1; then
        log_warning "  S3 æ–‡ä»¶å·²å­˜åœ¨ï¼Œè‡ªåŠ¨è¦†ç›–"
    fi
    
    # ä¸Šä¼ æ–‡ä»¶
    log_info "  ä¸Šä¼ ä¸­..."
    START_TIME=$(date +%s)
    
    if aws s3 cp "$LOCAL_PATH" "$S3_PATH" --region "$REGION" 2>&1 | tee /tmp/s3_upload.log; then
        END_TIME=$(date +%s)
        DURATION=$((END_TIME - START_TIME))
        
        # è®¡ç®—ä¸Šä¼ é€Ÿåº¦
        if [ "$DURATION" -gt 0 ] && [ "$FILE_SIZE" -gt 0 ]; then
            SPEED_MBPS=$(echo "scale=2; $FILE_SIZE_MB / $DURATION" | bc 2>/dev/null || echo "0")
            log_success "  ä¸Šä¼ å®Œæˆ: ${FILE_SIZE_MB} MB (è€—æ—¶: ${DURATION}s, é€Ÿåº¦: ${SPEED_MBPS} MB/s)"
        else
            log_success "  ä¸Šä¼ å®Œæˆ: ${FILE_SIZE_MB} MB"
        fi
        
        SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
    else
        log_error "  ä¸Šä¼ å¤±è´¥: $FILE"
        cat /tmp/s3_upload.log 2>/dev/null | tail -5
        FAIL_COUNT=$((FAIL_COUNT + 1))
    fi
    
    echo ""
done

# æ€»ç»“
TOTAL_SIZE_MB=$(echo "scale=2; $TOTAL_SIZE / 1024 / 1024" | bc 2>/dev/null || echo "0")

echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
log_info "ä¸Šä¼ å®Œæˆï¼"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "âœ… æˆåŠŸ: $SUCCESS_COUNT"
echo "âš ï¸  è·³è¿‡: $SKIP_COUNT"
echo "âŒ å¤±è´¥: $FAIL_COUNT"
echo "ğŸ“Š æ€»å¤§å°: ${TOTAL_SIZE_MB} MB"
echo ""
echo "ğŸ“ S3 è·¯å¾„: s3://$BUCKET_NAME/$S3_TEST_DATA_PREFIX/"
echo ""
echo "å·²ä¸Šä¼ çš„æ–‡ä»¶ï¼š"
for FILE in "${FILES[@]}"; do
    S3_PATH="s3://$BUCKET_NAME/$S3_TEST_DATA_PREFIX/$FILE"
    if aws s3 ls "$S3_PATH" --region "$REGION" > /dev/null 2>&1; then
        echo "  âœ… $FILE"
    fi
done

echo ""
log_info "æ•°æ®ä¸Šä¼ å®Œæˆï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹ S3 è·¯å¾„è¿›è¡Œæµ‹è¯•ï¼š"
echo ""
echo "ScanWorker:"
echo "  s3://$BUCKET_NAME/$S3_TEST_DATA_PREFIX/ScanWorker_data.pxl"
echo ""
echo "PartitionWorker:"
echo "  s3://$BUCKET_NAME/$S3_TEST_DATA_PREFIX/PartitionWorker_data.pxl"
echo ""
echo "AggregationWorker:"
echo "  s3://$BUCKET_NAME/$S3_TEST_DATA_PREFIX/AggregationWorker_data.pxl"
echo ""
echo "BroadcastJoinWorker:"
echo "  å¤§è¡¨: s3://$BUCKET_NAME/$S3_TEST_DATA_PREFIX/BroadcastJoinWorker_data1.pxl"
echo "  å°è¡¨: s3://$BUCKET_NAME/$S3_TEST_DATA_PREFIX/BroadcastJoinWorker_data2.pxl"
echo ""
echo "PartitionedJoinWorker:"
echo "  å¤§è¡¨: s3://$BUCKET_NAME/$S3_TEST_DATA_PREFIX/PartitionedJoinWorker_data1.pxl"
echo "  å°è¡¨: s3://$BUCKET_NAME/$S3_TEST_DATA_PREFIX/PartitionedJoinWorker_data2.pxl"
echo ""

